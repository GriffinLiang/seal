{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangxinran/anaconda3/envs/kgva/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import os.path as op\n",
    "\n",
    "from info.data.utils import save_json, load_json, get_stat\n",
    "\n",
    "VAW_DIR = '/mnt/sdb/data/wangxinran/dataset/VAW/data'\n",
    "VAW_CLEAN_DIR = '/mnt/sdb/data/wangxinran/dataset/VAW/clean_data'\n",
    "\n",
    "if not op.exists(VAW_CLEAN_DIR):\n",
    "    os.mkdir(VAW_CLEAN_DIR)\n",
    "\n",
    "MAX_BOX_W = 50\n",
    "MAX_BOX_H = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Load VAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# atts: 620\n",
      "# objs: 2260\n",
      "# pairs: 34873\n",
      "# imgs: 260895\n"
     ]
    }
   ],
   "source": [
    "data_train = load_json(f'{VAW_DIR}/train_part1.json')\n",
    "data_train += load_json(f'{VAW_DIR}/train_part2.json')\n",
    "data_val = load_json(f'{VAW_DIR}/val.json')\n",
    "data_test = load_json(f'{VAW_DIR}/test.json')\n",
    "\n",
    "all_data = data_train + data_val + data_test\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Clean VAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Filter Small Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# atts: 620\n",
      "# objs: 2115\n",
      "# pairs: 32402\n",
      "# imgs: 197185\n"
     ]
    }
   ],
   "source": [
    "all_data = [x for x in all_data if x['instance_bbox'][2] >= MAX_BOX_W and x['instance_bbox'][3] >= MAX_BOX_H]\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "if not op.exists(op.join(VAW_DIR, 'all.json')):\n",
    "    save_json(op.join(VAW_DIR, 'all.json'), all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Merge object categories with similar meaning (this is followed from GraphEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_ss_name(obj):\n",
    "    # Function to get WordNet synset.\n",
    "    sss = wn.synsets(obj)\n",
    "    if len(sss) == 0:\n",
    "        ss = obj\n",
    "    else:\n",
    "        ss = sss[0].name()\n",
    "    return ss\n",
    "\n",
    "def similar(obj, group):\n",
    "    # Function to depluralize object name, then check if 'obj' and 'group'\n",
    "    # may be similar using their WordNet synsets.\n",
    "    ss = get_ss_name(obj)\n",
    "    lem_obj = wnl.lemmatize(obj, \"n\")\n",
    "\n",
    "    for o, n in group:\n",
    "        sso = get_ss_name(o)\n",
    "        if ss == sso:\n",
    "            return True\n",
    "        lem_o = wnl.lemmatize(o, \"n\")\n",
    "        if lem_obj == lem_o:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "groups = []\n",
    "obj2group = {}\n",
    "\n",
    "for obj in tqdm(cnt_obj):\n",
    "    found = False\n",
    "    for i, gr in enumerate(groups):\n",
    "        if similar(obj, gr):\n",
    "            found = True\n",
    "            gr.append((obj, cnt_obj[obj]))\n",
    "            obj2group[obj] = i\n",
    "    if not found:\n",
    "        groups.append([(obj, cnt_obj[obj])])\n",
    "        obj2group[obj] = len(groups) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start renaming each group with its most representative object category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ins in all_data:\n",
    "    obj = ins['object_name']\n",
    "    group_idx = obj2group[obj]\n",
    "    obj_rep = None\n",
    "    n = 0\n",
    "    for o, m in groups[group_idx]:\n",
    "        if m > n:\n",
    "            obj_rep = o\n",
    "            n = m\n",
    "    ins['object_name'] = obj_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for gr in groups:\n",
    "    if len(gr) > 2:\n",
    "        print(gr)\n",
    "        n += 1\n",
    "        if n >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 't-shirt' and 'tee-shirt' to 't shirt'. \n",
    "\n",
    "\n",
    "Why 't shirt'? Because 't shirt' can be indexed by WordNet when we want to merge these shirt objects together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for ins in all_data:\n",
    "    obj_name = ins['object_name']\n",
    "    if obj_name == 't-shirt' or obj_name == 'tee shirt':\n",
    "        ins['object_name'] = 't shirt'\n",
    "        n += 1\n",
    "print(f'# renamed imgs: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Remove bad attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad attributes.\n",
    "bad_attrs = set([\n",
    "    'light colored', 'dark colored', 'extended', 'close', 'blurry', 'still', 'stopped', # difficult or not relevant to intrinsic properties\n",
    "    'dark skinned', 'light skinned', 'asian', 'caucasian', # sensitive\n",
    "    'male', 'female', # sensitive\n",
    "    'worn', 'printed', 'waiting', 'daytime', 'used', 'wild', 'lined', # noisy or requires too much context\n",
    "    'lined up', 'interior', 'displayed', 'in the background', 'far away', 'for sale', 'out of focus', 'turning', # noisy or requires too much context\n",
    "    'water' # not attribute\n",
    "])\n",
    "\n",
    "n_pos = 0\n",
    "n_neg = 0\n",
    "\n",
    "for ins in all_data:\n",
    "    for i in range(len(ins['positive_attributes'])):\n",
    "        n_before = len(ins['positive_attributes'])\n",
    "        ins['positive_attributes'] = [attr for attr in ins['positive_attributes'] if attr not in bad_attrs]\n",
    "        n_after = len(ins['positive_attributes'])\n",
    "        n_pos += n_before - n_after\n",
    "    \n",
    "    for i in range(len(ins['negative_attributes'])):\n",
    "        n_before = len(ins['negative_attributes'])\n",
    "        ins['negative_attributes'] = [attr for attr in ins['negative_attributes'] if attr not in bad_attrs]\n",
    "        n_after = len(ins['negative_attributes'])\n",
    "        n_neg += n_before - n_after\n",
    "\n",
    "\n",
    "print(f'Remove {n_pos} positive annotations')\n",
    "print(f'Remove {n_neg} negative annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Reform relevant metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_parent_types = load_json(op.join(VAW_DIR, 'attribute_parent_types.json'))\n",
    "attribute_types = load_json(op.join(VAW_DIR, 'attribute_types.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Check if two files has some attributes (not in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_set = set()\n",
    "object_set = set()\n",
    "for data in all_data:\n",
    "    for attribute in data['positive_attributes']:\n",
    "        attribute_set.add(attribute)\n",
    "    for attribute in data['negative_attributes']:\n",
    "        attribute_set.add(attribute)\n",
    "    object_set.add(data['object_name'])\n",
    "\n",
    "num_not_exists = 0\n",
    "attribute_flag = {attribute: False for idx, attribute in enumerate(sorted(list(attribute_set)))}\n",
    "for attribute_type, attribute_names in attribute_types.items():\n",
    "    for attribute_name in attribute_names:\n",
    "        if attribute_name not in attribute_flag:\n",
    "            num_not_exists += 1\n",
    "        else:\n",
    "            attribute_flag[attribute_name] = True\n",
    "print('num redundant attributes: {}'.format(num_not_exists))\n",
    "attribute_not_exist = [k for k, v in attribute_flag.items() if v == False]\n",
    "print('redundant attributes not in the cleaned data:')\n",
    "print(attribute_not_exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the redundant attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_types = []\n",
    "for attribute_type, attribute_names in attribute_types.items():\n",
    "    new_attribute_names = []\n",
    "    for attribute_name in attribute_names:\n",
    "        if attribute_name in attribute_flag:\n",
    "            new_attribute_names.append(attribute_name)    \n",
    "    attribute_types[attribute_type] = new_attribute_names\n",
    "    if len(new_attribute_names) == 0:\n",
    "        del_types.append(attribute_type)\n",
    "    print(attribute_type, len(attribute_names), len(new_attribute_names))\n",
    "\n",
    "for x in del_types:\n",
    "    del attribute_types[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_index = {attribute: idx for idx, attribute in enumerate(sorted(list(attribute_set)))}\n",
    "object_index = {object_name: idx for idx, object_name in enumerate(sorted(list(object_set)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Resplit all data to train, val, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "exp_data = copy.deepcopy(all_data)\n",
    "print(f'# imgs before: {len(exp_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id_set(data):\n",
    "    out_set = set()\n",
    "    for ins in data:\n",
    "        out_set.add(ins['instance_id'])\n",
    "    return out_set\n",
    "\n",
    "train_id = find_id_set(data_train)\n",
    "val_id = find_id_set(data_val)\n",
    "test_id = find_id_set(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train = [x for x in exp_data if x['instance_id'] in train_id]\n",
    "exp_val = [x for x in exp_data if x['instance_id'] in val_id]\n",
    "exp_test = [x for x in exp_data if x['instance_id'] in test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_train)\n",
    "print(f'train set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_val)\n",
    "print(f'val set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_test)\n",
    "print(f'test set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the samples having no positive lables, after the deleting of some attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train = [x for x in exp_train if len(x['positive_attributes']) != 0]\n",
    "exp_val = [x for x in exp_val if len(x['positive_attributes']) != 0]\n",
    "exp_test = [x for x in exp_test if len(x['positive_attributes']) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_train)\n",
    "print(f'train set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "print(f'val set')\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_val)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "print(f'test set')\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_test)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/data/wangxinran/attachment/ml-zsl/vaw_dataset-main/clean_data'\n",
    "\n",
    "def save_json(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    return data\n",
    "save_json(op.join(save_dir, 'all.json'), exp_data)\n",
    "save_json(op.join(save_dir, 'train.json'), exp_train)\n",
    "save_json(op.join(save_dir, 'val.json'), exp_val)\n",
    "save_json(op.join(save_dir, 'test.json'), exp_test)\n",
    "save_json(op.join(save_dir, 'attribute_index.json'), attribute_index)\n",
    "save_json(op.join(save_dir, 'object_index.json'), object_index)\n",
    "save_json(op.join(save_dir, 'attribute_types.json'), attribute_types)\n",
    "\n",
    "\"Finish!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### head tail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d921343b48bcf3a603f00699acf0b548a456a2e32330b8057e78217b2b9a3ef2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
