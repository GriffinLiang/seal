{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import os.path as op\n",
    "\n",
    "from seal.data.utils import save_json, load_json, get_stat\n",
    "\n",
    "VAW_DIR = '../data/VAW/data'\n",
    "VAW_CLEAN_DIR = '../data/VAW/clean_data'\n",
    "\n",
    "if not op.exists(VAW_CLEAN_DIR):\n",
    "    os.mkdir(VAW_CLEAN_DIR)\n",
    "\n",
    "MAX_BOX_W = 50\n",
    "MAX_BOX_H = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Load VAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# atts: 620\n",
      "# objs: 2260\n",
      "# pairs: 34873\n",
      "# imgs: 260895\n"
     ]
    }
   ],
   "source": [
    "data_train = load_json(f'{VAW_DIR}/train_part1.json')\n",
    "data_train += load_json(f'{VAW_DIR}/train_part2.json')\n",
    "data_val = load_json(f'{VAW_DIR}/val.json')\n",
    "data_test = load_json(f'{VAW_DIR}/test.json')\n",
    "\n",
    "all_data = data_train + data_val + data_test\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Clean VAW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Filter Small Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# atts: 620\n",
      "# objs: 2115\n",
      "# pairs: 32402\n",
      "# imgs: 197185\n"
     ]
    }
   ],
   "source": [
    "all_data = [x for x in all_data if x['instance_bbox'][2] >= MAX_BOX_W and x['instance_bbox'][3] >= MAX_BOX_H]\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "if not op.exists(op.join(VAW_DIR, 'all.json')):\n",
    "    save_json(op.join(VAW_DIR, 'all.json'), all_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Merge object categories with similar meaning (this is followed from GraphEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2115/2115 [00:51<00:00, 41.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_ss_name(obj):\n",
    "    # Function to get WordNet synset.\n",
    "    sss = wn.synsets(obj)\n",
    "    if len(sss) == 0:\n",
    "        ss = obj\n",
    "    else:\n",
    "        ss = sss[0].name()\n",
    "    return ss\n",
    "\n",
    "def similar(obj, group):\n",
    "    # Function to depluralize object name, then check if 'obj' and 'group'\n",
    "    # may be similar using their WordNet synsets.\n",
    "    ss = get_ss_name(obj)\n",
    "    lem_obj = wnl.lemmatize(obj, \"n\")\n",
    "\n",
    "    for o, n in group:\n",
    "        sso = get_ss_name(o)\n",
    "        if ss == sso:\n",
    "            return True\n",
    "        lem_o = wnl.lemmatize(o, \"n\")\n",
    "        if lem_obj == lem_o:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "groups = []\n",
    "obj2group = {}\n",
    "\n",
    "for obj in tqdm(cnt_obj):\n",
    "    found = False\n",
    "    for i, gr in enumerate(groups):\n",
    "        if similar(obj, gr):\n",
    "            found = True\n",
    "            gr.append((obj, cnt_obj[obj]))\n",
    "            obj2group[obj] = i\n",
    "    if not found:\n",
    "        groups.append([(obj, cnt_obj[obj])])\n",
    "        obj2group[obj] = len(groups) - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start renaming each group with its most representative object category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ins in all_data:\n",
    "    obj = ins['object_name']\n",
    "    group_idx = obj2group[obj]\n",
    "    obj_rep = None\n",
    "    n = 0\n",
    "    for o, m in groups[group_idx]:\n",
    "        if m > n:\n",
    "            obj_rep = o\n",
    "            n = m\n",
    "    ins['object_name'] = obj_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('floor', 1819), ('flooring', 72), ('floors', 36)]\n",
      "[('railing', 342), ('rail', 158), ('rails', 47), ('railings', 31), ('runway', 42)]\n",
      "[('shrubs', 57), ('bushes', 317), ('shrub', 69), ('bush', 620)]\n",
      "[('place', 5), ('spot', 100), ('spots', 91)]\n",
      "[('rocks', 245), ('stone', 107), ('rock', 413), ('stones', 62)]\n",
      "[('car', 1827), ('cars', 199), ('automobile', 1)]\n",
      "[('patch', 86), ('patches', 10), ('speckles', 1)]\n",
      "[('motorcycle', 540), ('bike', 396), ('bikes', 24), ('motorcycles', 28)]\n",
      "[('phone', 133), ('telephone', 11), ('phones', 1)]\n",
      "[('airplane', 354), ('plane', 596), ('airplanes', 18), ('aeroplane', 1), ('planes', 41)]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for gr in groups:\n",
    "    if len(gr) > 2:\n",
    "        print(gr)\n",
    "        n += 1\n",
    "        if n >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# atts: 620\n",
      "# objs: 1765\n",
      "# pairs: 27840\n",
      "# imgs: 197185\n"
     ]
    }
   ],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 't-shirt' and 'tee-shirt' to 't shirt'. \n",
    "\n",
    "\n",
    "Why 't shirt'? Because 't shirt' can be indexed by WordNet when we want to merge these shirt objects together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# renamed imgs: 419\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for ins in all_data:\n",
    "    obj_name = ins['object_name']\n",
    "    if obj_name == 't-shirt' or obj_name == 'tee shirt':\n",
    "        ins['object_name'] = 't shirt'\n",
    "        n += 1\n",
    "print(f'# renamed imgs: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# atts: 620\n",
      "# objs: 1763\n",
      "# pairs: 27810\n",
      "# imgs: 197185\n"
     ]
    }
   ],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Remove bad attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove 10871 positive annotations\n",
      "Remove 17344 negative annotations\n"
     ]
    }
   ],
   "source": [
    "# Remove bad attributes.\n",
    "bad_attrs = set([\n",
    "    'light colored', 'dark colored', 'extended', 'close', 'blurry', 'still', 'stopped', # difficult or not relevant to intrinsic properties\n",
    "    'dark skinned', 'light skinned', 'asian', 'caucasian', # sensitive\n",
    "    'male', 'female', # sensitive\n",
    "    'worn', 'printed', 'waiting', 'daytime', 'used', 'wild', 'lined', # noisy or requires too much context\n",
    "    'lined up', 'interior', 'displayed', 'in the background', 'far away', 'for sale', 'out of focus', 'turning', # noisy or requires too much context\n",
    "    'water' # not attribute\n",
    "])\n",
    "\n",
    "n_pos = 0\n",
    "n_neg = 0\n",
    "\n",
    "for ins in all_data:\n",
    "    for i in range(len(ins['positive_attributes'])):\n",
    "        n_before = len(ins['positive_attributes'])\n",
    "        ins['positive_attributes'] = [attr for attr in ins['positive_attributes'] if attr not in bad_attrs]\n",
    "        n_after = len(ins['positive_attributes'])\n",
    "        n_pos += n_before - n_after\n",
    "    \n",
    "    for i in range(len(ins['negative_attributes'])):\n",
    "        n_before = len(ins['negative_attributes'])\n",
    "        ins['negative_attributes'] = [attr for attr in ins['negative_attributes'] if attr not in bad_attrs]\n",
    "        n_after = len(ins['negative_attributes'])\n",
    "        n_neg += n_before - n_after\n",
    "\n",
    "\n",
    "print(f'Remove {n_pos} positive annotations')\n",
    "print(f'Remove {n_neg} negative annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# atts: 591\n",
      "# objs: 1763\n",
      "# pairs: 26382\n",
      "# imgs: 197185\n"
     ]
    }
   ],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(all_data)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Reform relevant metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_parent_types = load_json(op.join(VAW_DIR, 'attribute_parent_types.json'))\n",
    "attribute_types = load_json(op.join(VAW_DIR, 'attribute_types.json'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Check if two files has some attributes (not in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num redundant attributes: 76\n",
      "redundant attributes not in the cleaned data:\n",
      "['abandoned', 'almost empty', 'backless', 'being used', 'bell shaped', 'bushy', 'chipped', 'crossing arms', 'diagonal', 'dressed', 'holed', 'muscular', 'neat', 'numbered', 'nylon', 'overgrown', 'partially visible', 'perched', 'pulled back', 'raising arm', 'ridged', 'side view', 'staring', 'translucent', 'turned off', 'unmade', 'using phone', 'wearing bow tie', 'wearing headband', 'wheeled', 'white framed', 'wool']\n"
     ]
    }
   ],
   "source": [
    "attribute_set = set()\n",
    "object_set = set()\n",
    "for data in all_data:\n",
    "    for attribute in data['positive_attributes']:\n",
    "        attribute_set.add(attribute)\n",
    "    for attribute in data['negative_attributes']:\n",
    "        attribute_set.add(attribute)\n",
    "    object_set.add(data['object_name'])\n",
    "\n",
    "num_not_exists = 0\n",
    "attribute_flag = {attribute: False for idx, attribute in enumerate(sorted(list(attribute_set)))}\n",
    "for attribute_type, attribute_names in attribute_types.items():\n",
    "    for attribute_name in attribute_names:\n",
    "        if attribute_name not in attribute_flag:\n",
    "            num_not_exists += 1\n",
    "        else:\n",
    "            attribute_flag[attribute_name] = True\n",
    "print('num redundant attributes: {}'.format(num_not_exists))\n",
    "attribute_not_exist = [k for k, v in attribute_flag.items() if v == False]\n",
    "print('redundant attributes not in the cleaned data:')\n",
    "print(attribute_not_exist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the redundant attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color 61 58\n",
      "letter color 3 3\n",
      "hair color 6 6\n",
      "skin color 2 0\n",
      "wearing color 7 7\n",
      "tone 4 1\n",
      "color quantity 6 6\n",
      "brightness 2 2\n",
      "height 4 4\n",
      "length 2 2\n",
      "width 5 4\n",
      "fatness 5 5\n",
      "size 11 9\n",
      "thickness 2 2\n",
      "depth 2 2\n",
      "size comparison 2 0\n",
      "material 52 48\n",
      "shape 25 25\n",
      "orientation 6 6\n",
      "pattern 14 13\n",
      "closeness 2 0\n",
      "face expression 13 13\n",
      "hand movement 4 4\n",
      "pose 14 13\n",
      "activity 33 28\n",
      "sport activity 11 11\n",
      "face pose 4 3\n",
      "weather 10 9\n",
      "location 2 2\n",
      "place 6 4\n",
      "maturity 5 5\n",
      "newness 3 3\n",
      "cleanliness 4 4\n",
      "hardness 2 2\n",
      "weight 2 1\n",
      "race 2 0\n",
      "opaqeness 3 3\n",
      "gender 2 0\n",
      "texture 7 7\n",
      "state 11 11\n",
      "wearing accessories 6 6\n",
      "other 285 244\n"
     ]
    }
   ],
   "source": [
    "del_types = []\n",
    "for attribute_type, attribute_names in attribute_types.items():\n",
    "    new_attribute_names = []\n",
    "    for attribute_name in attribute_names:\n",
    "        if attribute_name in attribute_flag:\n",
    "            new_attribute_names.append(attribute_name)    \n",
    "    attribute_types[attribute_type] = new_attribute_names\n",
    "    if len(new_attribute_names) == 0:\n",
    "        del_types.append(attribute_type)\n",
    "    print(attribute_type, len(attribute_names), len(new_attribute_names))\n",
    "\n",
    "for x in del_types:\n",
    "    del attribute_types[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_index = {attribute: idx for idx, attribute in enumerate(sorted(list(attribute_set)))}\n",
    "object_index = {object_name: idx for idx, object_name in enumerate(sorted(list(object_set)))}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Resplit all data to train, val, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# imgs before: 197185\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "exp_data = copy.deepcopy(all_data)\n",
    "print(f'# imgs before: {len(exp_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id_set(data):\n",
    "    out_set = set()\n",
    "    for ins in data:\n",
    "        out_set.add(ins['instance_id'])\n",
    "    return out_set\n",
    "\n",
    "train_id = find_id_set(data_train)\n",
    "val_id = find_id_set(data_val)\n",
    "test_id = find_id_set(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train = [x for x in exp_data if x['instance_id'] in train_id]\n",
    "exp_val = [x for x in exp_data if x['instance_id'] in val_id]\n",
    "exp_test = [x for x in exp_data if x['instance_id'] in test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "# atts: 591\n",
      "# objs: 1642\n",
      "# pairs: 22709\n",
      "# imgs: 163651\n",
      "val set\n",
      "# atts: 576\n",
      "# objs: 766\n",
      "# pairs: 6950\n",
      "# imgs: 9180\n",
      "test set\n",
      "# atts: 591\n",
      "# objs: 977\n",
      "# pairs: 12697\n",
      "# imgs: 24354\n"
     ]
    }
   ],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_train)\n",
    "print(f'train set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_val)\n",
    "print(f'val set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_test)\n",
    "print(f'test set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the samples having no positive lables, after the deleting of some attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train = [x for x in exp_train if len(x['positive_attributes']) != 0]\n",
    "exp_val = [x for x in exp_val if len(x['positive_attributes']) != 0]\n",
    "exp_test = [x for x in exp_test if len(x['positive_attributes']) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "# atts: 591\n",
      "# objs: 1619\n",
      "# pairs: 22709\n",
      "# imgs: 141104\n",
      "val set\n",
      "# atts: 576\n",
      "# objs: 750\n",
      "# pairs: 6950\n",
      "# imgs: 8221\n",
      "test set\n",
      "# atts: 591\n",
      "# objs: 955\n",
      "# pairs: 12697\n",
      "# imgs: 21082\n"
     ]
    }
   ],
   "source": [
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_train)\n",
    "print(f'train set')\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "print(f'val set')\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_val)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')\n",
    "\n",
    "print(f'test set')\n",
    "cnt_attr, cnt_obj, cnt_pair, cooc, obj_afford, obj_afford_cooc, n_images = get_stat(exp_test)\n",
    "print(f'# atts: {len(cnt_attr)}')\n",
    "print(f'# objs: {len(cnt_obj)}')\n",
    "print(f'# pairs: {len(cnt_pair)}')\n",
    "print(f'# imgs: {n_images}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finish!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = VAW_CLEAN_DIR\n",
    "\n",
    "def save_json(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    return data\n",
    "save_json(op.join(save_dir, 'all.json'), exp_data)\n",
    "save_json(op.join(save_dir, 'train.json'), exp_train)\n",
    "save_json(op.join(save_dir, 'val.json'), exp_val)\n",
    "save_json(op.join(save_dir, 'test.json'), exp_test)\n",
    "save_json(op.join(save_dir, 'attribute_index.json'), attribute_index)\n",
    "save_json(op.join(save_dir, 'object_index.json'), object_index)\n",
    "save_json(op.join(save_dir, 'attribute_types.json'), attribute_types)\n",
    "\n",
    "\"Finish!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d921343b48bcf3a603f00699acf0b548a456a2e32330b8057e78217b2b9a3ef2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
